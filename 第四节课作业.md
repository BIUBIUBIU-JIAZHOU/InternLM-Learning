# 第四节课作业

## 微调单模态模型

尝试使用Xtuner微调1.8b的InternLM2-chat模型

### 准备环境

和前面的作业差不多，就不再赘述

### 准备数据和模型

准备数据，训练池恩小助手

![image-20240419213025408](assets/image-20240419213025408.png)

直接从服务器里复制internlm2-chat-1_8b到自己项目下面

![image-20240419220030423](assets/image-20240419220030423.png)

### 训练模型

xtuner一行命令启动训练

训练完成的模型将保存在/root/ft/train文件夹下

![image-20240420151324760](assets/image-20240420151324760.png)

常规训练已完成，保存了三个ch

![image-20240420155945917](assets/image-20240420155945917.png)

使用deepspeed加速训练，保存一个ch

![image-20240420161531037](assets/image-20240420161531037.png)

在训练之前可以调整以下参数，我调整了batch_size加速模型训练的速度，其他的参数最好不要调整，以免影响模型的性能

![image-20240420162802930](assets/image-20240420162802930.png)

### 转换、整合、测试和部署模型

将常规训练中的iter_768.pth转化为.bin格式的文件，也就是转化成Hugginface格式的文件

转化的命令和结果如下

![image-20240420164010023](assets/image-20240420164010023.png)

由于上述的训练是qlora训练，所以需要将训练好的模型参数和原始模型融合

转化的命令和结果如下

![image-20240420164737664](assets/image-20240420164737664.png)

用训练并且整合完成的模型进行推理

推理命令和结果如下，和教程里说明的情况一样，模型果然已经过拟合了

![image-20240420175001893](assets/image-20240420175001893.png)

最后，尝试将模型部署在web端，如下图所示

![image-20240420202452405](assets/image-20240420202452405.png)

## 微调多模态模型

### 准备数据和训练脚本

![image-20240420205942513](assets/image-20240420205942513.png)

### 微调模型

![image-20240420212413160](assets/image-20240420212413160.png)

在llava下保存了一个ch

![image-20240420213005529](assets/image-20240420213005529.png)

### 部署未经过多模态微调的模型

先进行模型转化，将pth格式的模型转为bin格式，如何部署，并提问如下图所示

可见该模型无法理解图片的内容

![image-20240420215323180](assets/image-20240420215323180.png)

### 部署经过多模态微调的模型

同上步骤一样

很明显，经过多模态微调的模型可以理解给定图片的内容，这是输入多模态，输出单模态，不知道是否可以输入输出都使用多模态？

![image-20240421174458856](assets/image-20240421174458856.png)



